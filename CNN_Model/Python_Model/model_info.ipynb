{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726fa9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor                                         N \t\t\t\t    Scale \t\t\t\t ZeroPt\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "serving_default_keras_tensor_30:0         120000 \t\t\t\t 0.003921568859368563 \t\t\t\t      0\n",
      "arith.constant                                 2 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst                             29 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst1                         14848 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst2                           512 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst3                        37748736 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst4                           512 \t\t\t\t 0.005283291917294264 \t\t\t\t    113\n",
      "tfl.pseudo_qconst5                           512 \t\t\t\t 0.1490822583436966 \t\t\t\t   -128\n",
      "tfl.pseudo_qconst6                           512 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst7                        2359296 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst8                           512 \t\t\t\t 0.003270635614171624 \t\t\t\t     70\n",
      "tfl.pseudo_qconst9                           512 \t\t\t\t 0.0017053347546607256 \t\t\t\t   -128\n",
      "tfl.pseudo_qconst10                          512 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst11                       1179648 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst12                          256 \t\t\t\t 0.003829863155260682 \t\t\t\t     87\n",
      "tfl.pseudo_qconst13                          256 \t\t\t\t 0.09097450226545334 \t\t\t\t   -128\n",
      "tfl.pseudo_qconst14                          256 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst15                       589824 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst16                          256 \t\t\t\t 0.008151660673320293 \t\t\t\t     78\n",
      "tfl.pseudo_qconst17                          256 \t\t\t\t 0.0010407568188384175 \t\t\t\t   -128\n",
      "tfl.pseudo_qconst18                          256 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst19                       294912 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst20                          128 \t\t\t\t 0.004105701111257076 \t\t\t\t    127\n",
      "tfl.pseudo_qconst21                          128 \t\t\t\t 0.001115808729082346 \t\t\t\t   -128\n",
      "tfl.pseudo_qconst22                          128 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst23                       147456 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst24                          128 \t\t\t\t 0.004118925426155329 \t\t\t\t    109\n",
      "tfl.pseudo_qconst25                          128 \t\t\t\t 0.001440622960217297 \t\t\t\t   -128\n",
      "tfl.pseudo_qconst26                          128 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst27                        73728 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst28                           64 \t\t\t\t 0.0061034648679196835 \t\t\t\t    127\n",
      "tfl.pseudo_qconst29                           64 \t\t\t\t 0.010403404012322426 \t\t\t\t   -128\n",
      "tfl.pseudo_qconst30                           64 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst31                        36864 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst32                           64 \t\t\t\t 0.02051985077559948 \t\t\t\t    107\n",
      "tfl.pseudo_qconst33                           64 \t\t\t\t 0.17310108244419098 \t\t\t\t   -128\n",
      "tfl.pseudo_qconst34                           64 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.pseudo_qconst35                         1728 \t\t\t\t      0.0 \t\t\t\t      0\n",
      "tfl.quantize                              120000 \t\t\t\t 0.003921568859368563 \t\t\t\t   -128\n",
      "sequential_1_1/sequential_1/conv2d_1/Relu;sequential_1_1/sequential_1/conv2d_1/BiasAdd;sequential_1_1/sequential_1/conv2d_1/convolution;1  2560000 \t\t\t\t 0.14930161833763123 \t\t\t\t   -128\n",
      "sequential_1_1/sequential_1/batch_normalization_1/batchnorm/mul_1  2560000 \t\t\t\t 0.1682112067937851 \t\t\t\t    -99\n",
      "sequential_1_1/sequential_1/batch_normalization_1/batchnorm/add_1  2560000 \t\t\t\t 0.16808736324310303 \t\t\t\t    -99\n",
      "sequential_1_1/sequential_1/conv2d_1_2/Relu;sequential_1_1/sequential_1/conv2d_1_2/BiasAdd;sequential_1_1/sequential_1/conv2d_1_2/convolution;sequential_1_1/sequential_1/conv2d_1_2/Squeeze  2560000 \t\t\t\t 0.4822568893432617 \t\t\t\t   -128\n",
      "sequential_1_1/sequential_1/batch_normalization_1_2/batchnorm/mul_1  2560000 \t\t\t\t 0.48836034536361694 \t\t\t\t   -125\n",
      "sequential_1_1/sequential_1/batch_normalization_1_2/batchnorm/add_1  2560000 \t\t\t\t 0.487743616104126 \t\t\t\t   -125\n",
      "sequential_1_1/sequential_1/max_pooling2d_1/MaxPool2d  640000 \t\t\t\t 0.487743616104126 \t\t\t\t   -125\n",
      "sequential_1_1/sequential_1/conv2d_2_1/Relu;sequential_1_1/sequential_1/conv2d_2_1/BiasAdd;sequential_1_1/sequential_1/conv2d_2_1/convolution;  1280000 \t\t\t\t 0.5958064198493958 \t\t\t\t   -128\n",
      "sequential_1_1/sequential_1/batch_normalization_2_1/batchnorm/mul_1  1280000 \t\t\t\t 0.1463908702135086 \t\t\t\t   -121\n",
      "sequential_1_1/sequential_1/batch_normalization_2_1/batchnorm/add_1  1280000 \t\t\t\t 0.14274635910987854 \t\t\t\t   -121\n",
      "sequential_1_1/sequential_1/conv2d_3_1/Relu;sequential_1_1/sequential_1/conv2d_3_1/BiasAdd;sequential_1_1/sequential_1/conv2d_3_1/convolution;  1280000 \t\t\t\t 0.9574838280677795 \t\t\t\t   -128\n",
      "sequential_1_1/sequential_1/batch_normalization_3_1/batchnorm/mul_1  1280000 \t\t\t\t 0.09289126843214035 \t\t\t\t   -117\n",
      "sequential_1_1/sequential_1/batch_normalization_3_1/batchnorm/add_1  1280000 \t\t\t\t 0.09052053838968277 \t\t\t\t   -116\n",
      "sequential_1_1/sequential_1/max_pooling2d_1_2/MaxPool2d  320000 \t\t\t\t 0.09052053838968277 \t\t\t\t   -116\n",
      "sequential_1_1/sequential_1/conv2d_4_1/Relu;sequential_1_1/sequential_1/conv2d_4_1/BiasAdd;sequential_1_1/sequential_1/conv2d_4_1/convolution;  640000 \t\t\t\t 0.9490053653717041 \t\t\t\t   -128\n",
      "sequential_1_1/sequential_1/batch_normalization_4_1/batchnorm/mul_1  640000 \t\t\t\t 0.11567893624305725 \t\t\t\t   -114\n",
      "sequential_1_1/sequential_1/batch_normalization_4_1/batchnorm/add_1  640000 \t\t\t\t 0.11381451785564423 \t\t\t\t   -113\n",
      "sequential_1_1/sequential_1/conv2d_5_1/Relu;sequential_1_1/sequential_1/conv2d_5_1/BiasAdd;sequential_1_1/sequential_1/conv2d_5_1/convolution;  640000 \t\t\t\t 1.2002378702163696 \t\t\t\t   -128\n",
      "sequential_1_1/sequential_1/batch_normalization_5_1/batchnorm/mul_1  640000 \t\t\t\t 0.3943060338497162 \t\t\t\t   -126\n",
      "sequential_1_1/sequential_1/batch_normalization_5_1/batchnorm/add_1  640000 \t\t\t\t 0.39389708638191223 \t\t\t\t   -126\n",
      "sequential_1_1/sequential_1/max_pooling2d_2_1/MaxPool2d  160000 \t\t\t\t 0.39389708638191223 \t\t\t\t   -126\n",
      "sequential_1_1/sequential_1/conv2d_6_1/Relu;sequential_1_1/sequential_1/conv2d_6_1/BiasAdd;sequential_1_1/sequential_1/conv2d_6_1/convolution;  320000 \t\t\t\t 1.5455434322357178 \t\t\t\t   -128\n",
      "sequential_1_1/sequential_1/batch_normalization_6_1/batchnorm/mul_1  320000 \t\t\t\t 0.15267038345336914 \t\t\t\t   -124\n",
      "sequential_1_1/sequential_1/batch_normalization_6_1/batchnorm/add_1  320000 \t\t\t\t 0.15163584053516388 \t\t\t\t   -124\n",
      "sequential_1_1/sequential_1/conv2d_7_1/Relu;sequential_1_1/sequential_1/conv2d_7_1/BiasAdd;sequential_1_1/sequential_1/conv2d_7_1/convolution;  320000 \t\t\t\t 1.0951658487319946 \t\t\t\t   -128\n",
      "sequential_1_1/sequential_1/batch_normalization_7_1/batchnorm/mul_1  320000 \t\t\t\t 0.6881277561187744 \t\t\t\t   -126\n",
      "sequential_1_1/sequential_1/batch_normalization_7_1/batchnorm/add_1  320000 \t\t\t\t 0.6877288222312927 \t\t\t\t   -126\n",
      "sequential_1_1/sequential_1/max_pooling2d_3_1/MaxPool2d   73728 \t\t\t\t 0.6877288222312927 \t\t\t\t   -126\n",
      "sequential_1_1/sequential_1/flatten_1/Reshape   73728 \t\t\t\t 0.6877288222312927 \t\t\t\t   -126\n",
      "sequential_1_1/sequential_1/dense_1/MatMul;sequential_1_1/sequential_1/dense_1/Relu;sequential_1_1/sequential_1/dense_1/BiasAdd     512 \t\t\t\t 4.46315336227417 \t\t\t\t   -128\n",
      "sequential_1_1/sequential_1/batch_normalization_8_1/batchnorm/mul_1;sequential_1_1/sequential_1/batch_normalization_8_1/batchnorm/add_1;sequential_1_1/sequential_1/logits_1/MatMul;sequential_1_1/sequential_1/logits_1/BiasAdd      29 \t\t\t\t 0.3223066031932831 \t\t\t\t    -18\n",
      "StatefulPartitionedCall_1:01                  29 \t\t\t\t 0.00390625 \t\t\t\t   -128\n",
      "StatefulPartitionedCall_1:0                   29 \t\t\t\t 0.00390625 \t\t\t\t      0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "interp = Interpreter(model_path=\"asl_model_quantized_int8.tflite\")\n",
    "interp.allocate_tensors()\n",
    "\n",
    "print(f\"{'Tensor':40s}  {'N':>6s} \\t\\t\\t\\t {'Scale':>8s} \\t\\t\\t\\t {'ZeroPt':>6s}\")\n",
    "print(\"-\"*166)\n",
    "for detail in interp.get_tensor_details():\n",
    "    name  = detail[\"name\"]\n",
    "    shape = detail[\"shape\"]\n",
    "    N     = int(np.prod(shape)) if shape is not None else 0\n",
    "\n",
    "    # Use the top‑level 'quantization' tuple when available\n",
    "    try:\n",
    "        scale, zero_point = detail[\"quantization\"]\n",
    "    except KeyError:\n",
    "        # Fallback to the parameters dict\n",
    "        scales = detail[\"quantization_parameters\"][\"scales\"]\n",
    "        zps    = detail[\"quantization_parameters\"][\"zero_points\"]\n",
    "        scale = float(scales[0])   if len(scales)  > 0 else None\n",
    "        zero_point = int(zps[0])   if len(zps)     > 0 else None\n",
    "\n",
    "    print(f\"{name:40s}  {N:6d} \\t\\t\\t\\t {str(scale):>8s} \\t\\t\\t\\t {str(zero_point):>6s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e90e47d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⚠️  Skipping sequential_1_1/sequential_1/conv2d_1/Relu;sequential_1_1/sequential_1/conv2d_1/BiasAdd;sequential_1_1/sequential_1/conv2d_1/convolution;1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_1/batchnorm/mul_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_1/batchnorm/add_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/conv2d_1_2/Relu;sequential_1_1/sequential_1/conv2d_1_2/BiasAdd;sequential_1_1/sequential_1/conv2d_1_2/convolution;sequential_1_1/sequential_1/conv2d_1_2/Squeeze: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_1_2/batchnorm/mul_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_1_2/batchnorm/add_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/max_pooling2d_1/MaxPool2d: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/conv2d_2_1/Relu;sequential_1_1/sequential_1/conv2d_2_1/BiasAdd;sequential_1_1/sequential_1/conv2d_2_1/convolution;: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_2_1/batchnorm/mul_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_2_1/batchnorm/add_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/conv2d_3_1/Relu;sequential_1_1/sequential_1/conv2d_3_1/BiasAdd;sequential_1_1/sequential_1/conv2d_3_1/convolution;: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_3_1/batchnorm/mul_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_3_1/batchnorm/add_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/max_pooling2d_1_2/MaxPool2d: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/conv2d_4_1/Relu;sequential_1_1/sequential_1/conv2d_4_1/BiasAdd;sequential_1_1/sequential_1/conv2d_4_1/convolution;: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_4_1/batchnorm/mul_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_4_1/batchnorm/add_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/conv2d_5_1/Relu;sequential_1_1/sequential_1/conv2d_5_1/BiasAdd;sequential_1_1/sequential_1/conv2d_5_1/convolution;: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_5_1/batchnorm/mul_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_5_1/batchnorm/add_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/max_pooling2d_2_1/MaxPool2d: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/conv2d_6_1/Relu;sequential_1_1/sequential_1/conv2d_6_1/BiasAdd;sequential_1_1/sequential_1/conv2d_6_1/convolution;: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_6_1/batchnorm/mul_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_6_1/batchnorm/add_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/conv2d_7_1/Relu;sequential_1_1/sequential_1/conv2d_7_1/BiasAdd;sequential_1_1/sequential_1/conv2d_7_1/convolution;: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_7_1/batchnorm/mul_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/batch_normalization_7_1/batchnorm/add_1: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/max_pooling2d_3_1/MaxPool2d: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/flatten_1/Reshape: no data buffer\n",
      "  ⚠️  Skipping sequential_1_1/sequential_1/dense_1/MatMul;sequential_1_1/sequential_1/dense_1/Relu;sequential_1_1/sequential_1/dense_1/BiasAdd: no data buffer\n",
      "✅ Wrote cpp_include/model_parameters.hpp with 250 lines\n"
     ]
    }
   ],
   "source": [
    "import json, numpy as np, re, os\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "def sanitize(name):\n",
    "    # turn “conv2d/weights:0” → “conv2d_weights_0”\n",
    "    return re.sub(r'[^0-9a-zA-Z_]', '_', name)\n",
    "\n",
    "# Load and prepare interpreter\n",
    "interp = Interpreter(model_path=\"asl_model_quantized_int8.tflite\")\n",
    "interp.allocate_tensors()\n",
    "\n",
    "# Header preamble\n",
    "os.makedirs(\"cpp_include\", exist_ok=True)\n",
    "hdr = [\n",
    "    \"#pragma once\",\n",
    "    \"#include <cstdint>\",\n",
    "    \"\",\n",
    "    \"// Auto‑generated model parameters\",\n",
    "]\n",
    "\n",
    "for detail in interp.get_tensor_details():\n",
    "    name   = detail[\"name\"]\n",
    "    scales = detail[\"quantization_parameters\"][\"scales\"]\n",
    "    # Only keep real int8‑quantized buffers\n",
    "    if scales.size == 0 or scales[0] == 0.0:\n",
    "        continue\n",
    "\n",
    "    # Try to grab the actual data; skip if no buffer allocated\n",
    "    idx = detail[\"index\"]\n",
    "    try:\n",
    "        arr = interp.get_tensor(idx)\n",
    "    except ValueError:\n",
    "        print(f\"  ⚠️  Skipping {name}: no data buffer\")\n",
    "        continue\n",
    "\n",
    "    arr = arr.flatten()\n",
    "    shape = detail[\"shape\"]\n",
    "    zps   = detail[\"quantization_parameters\"][\"zero_points\"]\n",
    "\n",
    "    sname = sanitize(name)\n",
    "    dtype = arr.dtype\n",
    "\n",
    "    # emit quant params\n",
    "    hdr.append(f\"// --- tensor: {name}\")\n",
    "    hdr.append(f\"static constexpr float {sname}_scale      = {float(scales[0]):.8g}f;\")\n",
    "    hdr.append(f\"static constexpr int32_t {sname}_zero_point = {int(zps[0])};\")\n",
    "    # emit shape & size\n",
    "    dims = \",\".join(str(d) for d in shape)\n",
    "    hdr.append(f\"static constexpr int {sname}_shape[] = {{{dims}}};\")\n",
    "    hdr.append(f\"static constexpr int {sname}_size  = {arr.size};\")\n",
    "    # emit raw data\n",
    "    if dtype == np.int8:\n",
    "        cpp_t = \"int8_t\"\n",
    "    elif dtype == np.uint8:\n",
    "        cpp_t = \"uint8_t\"\n",
    "    else:\n",
    "        cpp_t = \"int32_t\"\n",
    "    vals = \",\".join(str(int(x)) for x in arr)\n",
    "    hdr.append(f\"static const {cpp_t} {sname}_data[] = {{ {vals} }};\\n\")\n",
    "\n",
    "# Write the header\n",
    "with open(\"cpp_include/model_parameters.hpp\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(hdr))\n",
    "\n",
    "print(\"✅ Wrote cpp_include/model_parameters.hpp with\", len(hdr), \"lines\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8913bccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input round‑trip: True\n",
      "[[0.         0.         0.99609375 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donald/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "/home/donald/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:465: UserWarning: Warning: Enabling `experimental_preserve_all_tensors` with the BUILTIN or AUTO op resolver is intended for debugging purposes only. Be aware that this can significantly increase memory usage by storing all intermediate tensors. If you encounter memory problems or are not actively debugging, consider disabling this option.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "# 1) Load & prepare the interpreter\n",
    "interp = Interpreter(\n",
    "    model_path=\"asl_model_quantized_int8.tflite\",\n",
    "    experimental_preserve_all_tensors=True\n",
    ")\n",
    "interp.allocate_tensors()\n",
    "\n",
    "# 2) Load your JPG and resize to 200×200 RGB\n",
    "img = Image.open(\"asl_alphabet_test/C_test.jpg\").convert(\"RGB\")\n",
    "img = img.resize((200, 200), Image.BILINEAR)\n",
    "\n",
    "# 3) Turn into a (1,200,200,3) uint8 array\n",
    "arr = np.array(img, dtype=np.uint8)\n",
    "arr = arr[np.newaxis, ...]   # add batch dimension → shape (1,200,200,3)\n",
    "\n",
    "# 4) Find the input tensor index & set it\n",
    "input_details = interp.get_input_details()\n",
    "idx = input_details[0][\"index\"]          # usually 0\n",
    "# (you can also check input_details[0][\"shape\"] to confirm)\n",
    "interp.set_tensor(idx, arr)\n",
    "\n",
    "# 5) Invoke & then dump any tensor you like\n",
    "interp.invoke()\n",
    "out  = interp.get_tensor(input_details[0][\"index\"])  # just to verify\n",
    "print(\"input round‑trip:\", np.array_equal(out, arr))\n",
    "output_details = interp.get_output_details()\n",
    "\n",
    "out_info   = output_details[0]\n",
    "out_index  = out_info[\"index\"]\n",
    "q_output   = interp.get_tensor(out_index)           # e.g. shape (1,N), dtype=int8/uint8\n",
    "scale, zp  = out_info[\"quantization\"]               # float scale and int zero‑point\n",
    "# convert back to “real” floats if you like:\n",
    "real_output = scale * (q_output.astype(np.float32) - zp)\n",
    "print(real_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7589e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution steps (pure‑TFLite CPU ops):\n",
      "\n",
      "Step  0: QUANTIZE\n",
      "    IN  tensor #0: “serving_default_keras_tensor_30:0” shape=(1, 200, 200, 3), type=uint8, scale=0.003921568859368563, zp=0\n",
      "    OUT tensor #38: “tfl.quantize” shape=(1, 200, 200, 3), type=int8, scale=0.003921568859368563, zp=-128\n",
      "\n",
      "Step  1: CONV_2D\n",
      "    IN  tensor #38: “tfl.quantize” shape=(1, 200, 200, 3), type=int8, scale=0.003921568859368563, zp=-128\n",
      "    IN  tensor #37: “tfl.pseudo_qconst35” shape=(64, 3, 3, 3), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #36: “tfl.pseudo_qconst34” shape=(64,), type=int32, scale=0.0, zp=0\n",
      "    OUT tensor #39: “sequential_1_1/sequential_1/conv2d_1/Relu;sequential_1_1/sequential_1/conv2d_1/BiasAdd;sequential_1_1/sequential_1/conv2d_1/convolution;1” shape=(1, 200, 200, 64), type=int8, scale=0.14930161833763123, zp=-128\n",
      "\n",
      "Step  2: MUL\n",
      "    IN  tensor #39: “sequential_1_1/sequential_1/conv2d_1/Relu;sequential_1_1/sequential_1/conv2d_1/BiasAdd;sequential_1_1/sequential_1/conv2d_1/convolution;1” shape=(1, 200, 200, 64), type=int8, scale=0.14930161833763123, zp=-128\n",
      "    IN  tensor #35: “tfl.pseudo_qconst33” shape=(64,), type=int8, scale=0.17310108244419098, zp=-128\n",
      "    OUT tensor #40: “sequential_1_1/sequential_1/batch_normalization_1/batchnorm/mul_1” shape=(1, 200, 200, 64), type=int8, scale=0.1682112067937851, zp=-99\n",
      "\n",
      "Step  3: ADD\n",
      "    IN  tensor #40: “sequential_1_1/sequential_1/batch_normalization_1/batchnorm/mul_1” shape=(1, 200, 200, 64), type=int8, scale=0.1682112067937851, zp=-99\n",
      "    IN  tensor #34: “tfl.pseudo_qconst32” shape=(64,), type=int8, scale=0.02051985077559948, zp=107\n",
      "    OUT tensor #41: “sequential_1_1/sequential_1/batch_normalization_1/batchnorm/add_1” shape=(1, 200, 200, 64), type=int8, scale=0.16808736324310303, zp=-99\n",
      "\n",
      "Step  4: CONV_2D\n",
      "    IN  tensor #41: “sequential_1_1/sequential_1/batch_normalization_1/batchnorm/add_1” shape=(1, 200, 200, 64), type=int8, scale=0.16808736324310303, zp=-99\n",
      "    IN  tensor #33: “tfl.pseudo_qconst31” shape=(64, 3, 3, 64), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #32: “tfl.pseudo_qconst30” shape=(64,), type=int32, scale=0.0, zp=0\n",
      "    OUT tensor #42: “sequential_1_1/sequential_1/conv2d_1_2/Relu;sequential_1_1/sequential_1/conv2d_1_2/BiasAdd;sequential_1_1/sequential_1/conv2d_1_2/convolution;sequential_1_1/sequential_1/conv2d_1_2/Squeeze” shape=(1, 200, 200, 64), type=int8, scale=0.4822568893432617, zp=-128\n",
      "\n",
      "Step  5: MUL\n",
      "    IN  tensor #42: “sequential_1_1/sequential_1/conv2d_1_2/Relu;sequential_1_1/sequential_1/conv2d_1_2/BiasAdd;sequential_1_1/sequential_1/conv2d_1_2/convolution;sequential_1_1/sequential_1/conv2d_1_2/Squeeze” shape=(1, 200, 200, 64), type=int8, scale=0.4822568893432617, zp=-128\n",
      "    IN  tensor #31: “tfl.pseudo_qconst29” shape=(64,), type=int8, scale=0.010403404012322426, zp=-128\n",
      "    OUT tensor #43: “sequential_1_1/sequential_1/batch_normalization_1_2/batchnorm/mul_1” shape=(1, 200, 200, 64), type=int8, scale=0.48836034536361694, zp=-125\n",
      "\n",
      "Step  6: ADD\n",
      "    IN  tensor #43: “sequential_1_1/sequential_1/batch_normalization_1_2/batchnorm/mul_1” shape=(1, 200, 200, 64), type=int8, scale=0.48836034536361694, zp=-125\n",
      "    IN  tensor #30: “tfl.pseudo_qconst28” shape=(64,), type=int8, scale=0.0061034648679196835, zp=127\n",
      "    OUT tensor #44: “sequential_1_1/sequential_1/batch_normalization_1_2/batchnorm/add_1” shape=(1, 200, 200, 64), type=int8, scale=0.487743616104126, zp=-125\n",
      "\n",
      "Step  7: MAX_POOL_2D\n",
      "    IN  tensor #44: “sequential_1_1/sequential_1/batch_normalization_1_2/batchnorm/add_1” shape=(1, 200, 200, 64), type=int8, scale=0.487743616104126, zp=-125\n",
      "    OUT tensor #45: “sequential_1_1/sequential_1/max_pooling2d_1/MaxPool2d” shape=(1, 100, 100, 64), type=int8, scale=0.487743616104126, zp=-125\n",
      "\n",
      "Step  8: CONV_2D\n",
      "    IN  tensor #45: “sequential_1_1/sequential_1/max_pooling2d_1/MaxPool2d” shape=(1, 100, 100, 64), type=int8, scale=0.487743616104126, zp=-125\n",
      "    IN  tensor #29: “tfl.pseudo_qconst27” shape=(128, 3, 3, 64), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #28: “tfl.pseudo_qconst26” shape=(128,), type=int32, scale=0.0, zp=0\n",
      "    OUT tensor #46: “sequential_1_1/sequential_1/conv2d_2_1/Relu;sequential_1_1/sequential_1/conv2d_2_1/BiasAdd;sequential_1_1/sequential_1/conv2d_2_1/convolution;” shape=(1, 100, 100, 128), type=int8, scale=0.5958064198493958, zp=-128\n",
      "\n",
      "Step  9: MUL\n",
      "    IN  tensor #46: “sequential_1_1/sequential_1/conv2d_2_1/Relu;sequential_1_1/sequential_1/conv2d_2_1/BiasAdd;sequential_1_1/sequential_1/conv2d_2_1/convolution;” shape=(1, 100, 100, 128), type=int8, scale=0.5958064198493958, zp=-128\n",
      "    IN  tensor #27: “tfl.pseudo_qconst25” shape=(128,), type=int8, scale=0.001440622960217297, zp=-128\n",
      "    OUT tensor #47: “sequential_1_1/sequential_1/batch_normalization_2_1/batchnorm/mul_1” shape=(1, 100, 100, 128), type=int8, scale=0.1463908702135086, zp=-121\n",
      "\n",
      "Step 10: ADD\n",
      "    IN  tensor #47: “sequential_1_1/sequential_1/batch_normalization_2_1/batchnorm/mul_1” shape=(1, 100, 100, 128), type=int8, scale=0.1463908702135086, zp=-121\n",
      "    IN  tensor #26: “tfl.pseudo_qconst24” shape=(128,), type=int8, scale=0.004118925426155329, zp=109\n",
      "    OUT tensor #48: “sequential_1_1/sequential_1/batch_normalization_2_1/batchnorm/add_1” shape=(1, 100, 100, 128), type=int8, scale=0.14274635910987854, zp=-121\n",
      "\n",
      "Step 11: CONV_2D\n",
      "    IN  tensor #48: “sequential_1_1/sequential_1/batch_normalization_2_1/batchnorm/add_1” shape=(1, 100, 100, 128), type=int8, scale=0.14274635910987854, zp=-121\n",
      "    IN  tensor #25: “tfl.pseudo_qconst23” shape=(128, 3, 3, 128), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #24: “tfl.pseudo_qconst22” shape=(128,), type=int32, scale=0.0, zp=0\n",
      "    OUT tensor #49: “sequential_1_1/sequential_1/conv2d_3_1/Relu;sequential_1_1/sequential_1/conv2d_3_1/BiasAdd;sequential_1_1/sequential_1/conv2d_3_1/convolution;” shape=(1, 100, 100, 128), type=int8, scale=0.9574838280677795, zp=-128\n",
      "\n",
      "Step 12: MUL\n",
      "    IN  tensor #49: “sequential_1_1/sequential_1/conv2d_3_1/Relu;sequential_1_1/sequential_1/conv2d_3_1/BiasAdd;sequential_1_1/sequential_1/conv2d_3_1/convolution;” shape=(1, 100, 100, 128), type=int8, scale=0.9574838280677795, zp=-128\n",
      "    IN  tensor #23: “tfl.pseudo_qconst21” shape=(128,), type=int8, scale=0.001115808729082346, zp=-128\n",
      "    OUT tensor #50: “sequential_1_1/sequential_1/batch_normalization_3_1/batchnorm/mul_1” shape=(1, 100, 100, 128), type=int8, scale=0.09289126843214035, zp=-117\n",
      "\n",
      "Step 13: ADD\n",
      "    IN  tensor #50: “sequential_1_1/sequential_1/batch_normalization_3_1/batchnorm/mul_1” shape=(1, 100, 100, 128), type=int8, scale=0.09289126843214035, zp=-117\n",
      "    IN  tensor #22: “tfl.pseudo_qconst20” shape=(128,), type=int8, scale=0.004105701111257076, zp=127\n",
      "    OUT tensor #51: “sequential_1_1/sequential_1/batch_normalization_3_1/batchnorm/add_1” shape=(1, 100, 100, 128), type=int8, scale=0.09052053838968277, zp=-116\n",
      "\n",
      "Step 14: MAX_POOL_2D\n",
      "    IN  tensor #51: “sequential_1_1/sequential_1/batch_normalization_3_1/batchnorm/add_1” shape=(1, 100, 100, 128), type=int8, scale=0.09052053838968277, zp=-116\n",
      "    OUT tensor #52: “sequential_1_1/sequential_1/max_pooling2d_1_2/MaxPool2d” shape=(1, 50, 50, 128), type=int8, scale=0.09052053838968277, zp=-116\n",
      "\n",
      "Step 15: CONV_2D\n",
      "    IN  tensor #52: “sequential_1_1/sequential_1/max_pooling2d_1_2/MaxPool2d” shape=(1, 50, 50, 128), type=int8, scale=0.09052053838968277, zp=-116\n",
      "    IN  tensor #21: “tfl.pseudo_qconst19” shape=(256, 3, 3, 128), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #20: “tfl.pseudo_qconst18” shape=(256,), type=int32, scale=0.0, zp=0\n",
      "    OUT tensor #53: “sequential_1_1/sequential_1/conv2d_4_1/Relu;sequential_1_1/sequential_1/conv2d_4_1/BiasAdd;sequential_1_1/sequential_1/conv2d_4_1/convolution;” shape=(1, 50, 50, 256), type=int8, scale=0.9490053653717041, zp=-128\n",
      "\n",
      "Step 16: MUL\n",
      "    IN  tensor #53: “sequential_1_1/sequential_1/conv2d_4_1/Relu;sequential_1_1/sequential_1/conv2d_4_1/BiasAdd;sequential_1_1/sequential_1/conv2d_4_1/convolution;” shape=(1, 50, 50, 256), type=int8, scale=0.9490053653717041, zp=-128\n",
      "    IN  tensor #19: “tfl.pseudo_qconst17” shape=(256,), type=int8, scale=0.0010407568188384175, zp=-128\n",
      "    OUT tensor #54: “sequential_1_1/sequential_1/batch_normalization_4_1/batchnorm/mul_1” shape=(1, 50, 50, 256), type=int8, scale=0.11567893624305725, zp=-114\n",
      "\n",
      "Step 17: ADD\n",
      "    IN  tensor #54: “sequential_1_1/sequential_1/batch_normalization_4_1/batchnorm/mul_1” shape=(1, 50, 50, 256), type=int8, scale=0.11567893624305725, zp=-114\n",
      "    IN  tensor #18: “tfl.pseudo_qconst16” shape=(256,), type=int8, scale=0.008151660673320293, zp=78\n",
      "    OUT tensor #55: “sequential_1_1/sequential_1/batch_normalization_4_1/batchnorm/add_1” shape=(1, 50, 50, 256), type=int8, scale=0.11381451785564423, zp=-113\n",
      "\n",
      "Step 18: CONV_2D\n",
      "    IN  tensor #55: “sequential_1_1/sequential_1/batch_normalization_4_1/batchnorm/add_1” shape=(1, 50, 50, 256), type=int8, scale=0.11381451785564423, zp=-113\n",
      "    IN  tensor #17: “tfl.pseudo_qconst15” shape=(256, 3, 3, 256), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #16: “tfl.pseudo_qconst14” shape=(256,), type=int32, scale=0.0, zp=0\n",
      "    OUT tensor #56: “sequential_1_1/sequential_1/conv2d_5_1/Relu;sequential_1_1/sequential_1/conv2d_5_1/BiasAdd;sequential_1_1/sequential_1/conv2d_5_1/convolution;” shape=(1, 50, 50, 256), type=int8, scale=1.2002378702163696, zp=-128\n",
      "\n",
      "Step 19: MUL\n",
      "    IN  tensor #56: “sequential_1_1/sequential_1/conv2d_5_1/Relu;sequential_1_1/sequential_1/conv2d_5_1/BiasAdd;sequential_1_1/sequential_1/conv2d_5_1/convolution;” shape=(1, 50, 50, 256), type=int8, scale=1.2002378702163696, zp=-128\n",
      "    IN  tensor #15: “tfl.pseudo_qconst13” shape=(256,), type=int8, scale=0.09097450226545334, zp=-128\n",
      "    OUT tensor #57: “sequential_1_1/sequential_1/batch_normalization_5_1/batchnorm/mul_1” shape=(1, 50, 50, 256), type=int8, scale=0.3943060338497162, zp=-126\n",
      "\n",
      "Step 20: ADD\n",
      "    IN  tensor #57: “sequential_1_1/sequential_1/batch_normalization_5_1/batchnorm/mul_1” shape=(1, 50, 50, 256), type=int8, scale=0.3943060338497162, zp=-126\n",
      "    IN  tensor #14: “tfl.pseudo_qconst12” shape=(256,), type=int8, scale=0.003829863155260682, zp=87\n",
      "    OUT tensor #58: “sequential_1_1/sequential_1/batch_normalization_5_1/batchnorm/add_1” shape=(1, 50, 50, 256), type=int8, scale=0.39389708638191223, zp=-126\n",
      "\n",
      "Step 21: MAX_POOL_2D\n",
      "    IN  tensor #58: “sequential_1_1/sequential_1/batch_normalization_5_1/batchnorm/add_1” shape=(1, 50, 50, 256), type=int8, scale=0.39389708638191223, zp=-126\n",
      "    OUT tensor #59: “sequential_1_1/sequential_1/max_pooling2d_2_1/MaxPool2d” shape=(1, 25, 25, 256), type=int8, scale=0.39389708638191223, zp=-126\n",
      "\n",
      "Step 22: CONV_2D\n",
      "    IN  tensor #59: “sequential_1_1/sequential_1/max_pooling2d_2_1/MaxPool2d” shape=(1, 25, 25, 256), type=int8, scale=0.39389708638191223, zp=-126\n",
      "    IN  tensor #13: “tfl.pseudo_qconst11” shape=(512, 3, 3, 256), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #12: “tfl.pseudo_qconst10” shape=(512,), type=int32, scale=0.0, zp=0\n",
      "    OUT tensor #60: “sequential_1_1/sequential_1/conv2d_6_1/Relu;sequential_1_1/sequential_1/conv2d_6_1/BiasAdd;sequential_1_1/sequential_1/conv2d_6_1/convolution;” shape=(1, 25, 25, 512), type=int8, scale=1.5455434322357178, zp=-128\n",
      "\n",
      "Step 23: MUL\n",
      "    IN  tensor #60: “sequential_1_1/sequential_1/conv2d_6_1/Relu;sequential_1_1/sequential_1/conv2d_6_1/BiasAdd;sequential_1_1/sequential_1/conv2d_6_1/convolution;” shape=(1, 25, 25, 512), type=int8, scale=1.5455434322357178, zp=-128\n",
      "    IN  tensor #11: “tfl.pseudo_qconst9” shape=(512,), type=int8, scale=0.0017053347546607256, zp=-128\n",
      "    OUT tensor #61: “sequential_1_1/sequential_1/batch_normalization_6_1/batchnorm/mul_1” shape=(1, 25, 25, 512), type=int8, scale=0.15267038345336914, zp=-124\n",
      "\n",
      "Step 24: ADD\n",
      "    IN  tensor #61: “sequential_1_1/sequential_1/batch_normalization_6_1/batchnorm/mul_1” shape=(1, 25, 25, 512), type=int8, scale=0.15267038345336914, zp=-124\n",
      "    IN  tensor #10: “tfl.pseudo_qconst8” shape=(512,), type=int8, scale=0.003270635614171624, zp=70\n",
      "    OUT tensor #62: “sequential_1_1/sequential_1/batch_normalization_6_1/batchnorm/add_1” shape=(1, 25, 25, 512), type=int8, scale=0.15163584053516388, zp=-124\n",
      "\n",
      "Step 25: CONV_2D\n",
      "    IN  tensor #62: “sequential_1_1/sequential_1/batch_normalization_6_1/batchnorm/add_1” shape=(1, 25, 25, 512), type=int8, scale=0.15163584053516388, zp=-124\n",
      "    IN  tensor #9: “tfl.pseudo_qconst7” shape=(512, 3, 3, 512), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #8: “tfl.pseudo_qconst6” shape=(512,), type=int32, scale=0.0, zp=0\n",
      "    OUT tensor #63: “sequential_1_1/sequential_1/conv2d_7_1/Relu;sequential_1_1/sequential_1/conv2d_7_1/BiasAdd;sequential_1_1/sequential_1/conv2d_7_1/convolution;” shape=(1, 25, 25, 512), type=int8, scale=1.0951658487319946, zp=-128\n",
      "\n",
      "Step 26: MUL\n",
      "    IN  tensor #63: “sequential_1_1/sequential_1/conv2d_7_1/Relu;sequential_1_1/sequential_1/conv2d_7_1/BiasAdd;sequential_1_1/sequential_1/conv2d_7_1/convolution;” shape=(1, 25, 25, 512), type=int8, scale=1.0951658487319946, zp=-128\n",
      "    IN  tensor #7: “tfl.pseudo_qconst5” shape=(512,), type=int8, scale=0.1490822583436966, zp=-128\n",
      "    OUT tensor #64: “sequential_1_1/sequential_1/batch_normalization_7_1/batchnorm/mul_1” shape=(1, 25, 25, 512), type=int8, scale=0.6881277561187744, zp=-126\n",
      "\n",
      "Step 27: ADD\n",
      "    IN  tensor #64: “sequential_1_1/sequential_1/batch_normalization_7_1/batchnorm/mul_1” shape=(1, 25, 25, 512), type=int8, scale=0.6881277561187744, zp=-126\n",
      "    IN  tensor #6: “tfl.pseudo_qconst4” shape=(512,), type=int8, scale=0.005283291917294264, zp=113\n",
      "    OUT tensor #65: “sequential_1_1/sequential_1/batch_normalization_7_1/batchnorm/add_1” shape=(1, 25, 25, 512), type=int8, scale=0.6877288222312927, zp=-126\n",
      "\n",
      "Step 28: MAX_POOL_2D\n",
      "    IN  tensor #65: “sequential_1_1/sequential_1/batch_normalization_7_1/batchnorm/add_1” shape=(1, 25, 25, 512), type=int8, scale=0.6877288222312927, zp=-126\n",
      "    OUT tensor #66: “sequential_1_1/sequential_1/max_pooling2d_3_1/MaxPool2d” shape=(1, 12, 12, 512), type=int8, scale=0.6877288222312927, zp=-126\n",
      "\n",
      "Step 29: RESHAPE\n",
      "    IN  tensor #66: “sequential_1_1/sequential_1/max_pooling2d_3_1/MaxPool2d” shape=(1, 12, 12, 512), type=int8, scale=0.6877288222312927, zp=-126\n",
      "    IN  tensor #1: “arith.constant” shape=(2,), type=int32, scale=0.0, zp=0\n",
      "    OUT tensor #67: “sequential_1_1/sequential_1/flatten_1/Reshape” shape=(1, 73728), type=int8, scale=0.6877288222312927, zp=-126\n",
      "\n",
      "Step 30: FULLY_CONNECTED\n",
      "    IN  tensor #67: “sequential_1_1/sequential_1/flatten_1/Reshape” shape=(1, 73728), type=int8, scale=0.6877288222312927, zp=-126\n",
      "    IN  tensor #5: “tfl.pseudo_qconst3” shape=(512, 73728), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #4: “tfl.pseudo_qconst2” shape=(512,), type=int32, scale=0.0, zp=0\n",
      "    OUT tensor #68: “sequential_1_1/sequential_1/dense_1/MatMul;sequential_1_1/sequential_1/dense_1/Relu;sequential_1_1/sequential_1/dense_1/BiasAdd” shape=(1, 512), type=int8, scale=4.46315336227417, zp=-128\n",
      "\n",
      "Step 31: FULLY_CONNECTED\n",
      "    IN  tensor #68: “sequential_1_1/sequential_1/dense_1/MatMul;sequential_1_1/sequential_1/dense_1/Relu;sequential_1_1/sequential_1/dense_1/BiasAdd” shape=(1, 512), type=int8, scale=4.46315336227417, zp=-128\n",
      "    IN  tensor #3: “tfl.pseudo_qconst1” shape=(29, 512), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #2: “tfl.pseudo_qconst” shape=(29,), type=int32, scale=0.0, zp=0\n",
      "    OUT tensor #69: “sequential_1_1/sequential_1/batch_normalization_8_1/batchnorm/mul_1;sequential_1_1/sequential_1/batch_normalization_8_1/batchnorm/add_1;sequential_1_1/sequential_1/logits_1/MatMul;sequential_1_1/sequential_1/logits_1/BiasAdd” shape=(1, 29), type=int8, scale=0.3223066031932831, zp=-18\n",
      "\n",
      "Step 32: SOFTMAX\n",
      "    IN  tensor #69: “sequential_1_1/sequential_1/batch_normalization_8_1/batchnorm/mul_1;sequential_1_1/sequential_1/batch_normalization_8_1/batchnorm/add_1;sequential_1_1/sequential_1/logits_1/MatMul;sequential_1_1/sequential_1/logits_1/BiasAdd” shape=(1, 29), type=int8, scale=0.3223066031932831, zp=-18\n",
      "    OUT tensor #70: “StatefulPartitionedCall_1:01” shape=(1, 29), type=int8, scale=0.00390625, zp=-128\n",
      "\n",
      "Step 33: QUANTIZE\n",
      "    IN  tensor #70: “StatefulPartitionedCall_1:01” shape=(1, 29), type=int8, scale=0.00390625, zp=-128\n",
      "    OUT tensor #71: “StatefulPartitionedCall_1:0” shape=(1, 29), type=uint8, scale=0.00390625, zp=0\n",
      "\n",
      "Step 34: DELEGATE\n",
      "    IN  tensor #36: “tfl.pseudo_qconst34” shape=(64,), type=int32, scale=0.0, zp=0\n",
      "    IN  tensor #37: “tfl.pseudo_qconst35” shape=(64, 3, 3, 3), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #38: “tfl.quantize” shape=(1, 200, 200, 3), type=int8, scale=0.003921568859368563, zp=-128\n",
      "    OUT tensor #39: “sequential_1_1/sequential_1/conv2d_1/Relu;sequential_1_1/sequential_1/conv2d_1/BiasAdd;sequential_1_1/sequential_1/conv2d_1/convolution;1” shape=(1, 200, 200, 64), type=int8, scale=0.14930161833763123, zp=-128\n",
      "\n",
      "Step 35: DELEGATE\n",
      "    IN  tensor #35: “tfl.pseudo_qconst33” shape=(64,), type=int8, scale=0.17310108244419098, zp=-128\n",
      "    IN  tensor #39: “sequential_1_1/sequential_1/conv2d_1/Relu;sequential_1_1/sequential_1/conv2d_1/BiasAdd;sequential_1_1/sequential_1/conv2d_1/convolution;1” shape=(1, 200, 200, 64), type=int8, scale=0.14930161833763123, zp=-128\n",
      "    OUT tensor #40: “sequential_1_1/sequential_1/batch_normalization_1/batchnorm/mul_1” shape=(1, 200, 200, 64), type=int8, scale=0.1682112067937851, zp=-99\n",
      "\n",
      "Step 36: DELEGATE\n",
      "    IN  tensor #34: “tfl.pseudo_qconst32” shape=(64,), type=int8, scale=0.02051985077559948, zp=107\n",
      "    IN  tensor #40: “sequential_1_1/sequential_1/batch_normalization_1/batchnorm/mul_1” shape=(1, 200, 200, 64), type=int8, scale=0.1682112067937851, zp=-99\n",
      "    OUT tensor #41: “sequential_1_1/sequential_1/batch_normalization_1/batchnorm/add_1” shape=(1, 200, 200, 64), type=int8, scale=0.16808736324310303, zp=-99\n",
      "\n",
      "Step 37: DELEGATE\n",
      "    IN  tensor #32: “tfl.pseudo_qconst30” shape=(64,), type=int32, scale=0.0, zp=0\n",
      "    IN  tensor #33: “tfl.pseudo_qconst31” shape=(64, 3, 3, 64), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #41: “sequential_1_1/sequential_1/batch_normalization_1/batchnorm/add_1” shape=(1, 200, 200, 64), type=int8, scale=0.16808736324310303, zp=-99\n",
      "    OUT tensor #42: “sequential_1_1/sequential_1/conv2d_1_2/Relu;sequential_1_1/sequential_1/conv2d_1_2/BiasAdd;sequential_1_1/sequential_1/conv2d_1_2/convolution;sequential_1_1/sequential_1/conv2d_1_2/Squeeze” shape=(1, 200, 200, 64), type=int8, scale=0.4822568893432617, zp=-128\n",
      "\n",
      "Step 38: DELEGATE\n",
      "    IN  tensor #31: “tfl.pseudo_qconst29” shape=(64,), type=int8, scale=0.010403404012322426, zp=-128\n",
      "    IN  tensor #42: “sequential_1_1/sequential_1/conv2d_1_2/Relu;sequential_1_1/sequential_1/conv2d_1_2/BiasAdd;sequential_1_1/sequential_1/conv2d_1_2/convolution;sequential_1_1/sequential_1/conv2d_1_2/Squeeze” shape=(1, 200, 200, 64), type=int8, scale=0.4822568893432617, zp=-128\n",
      "    OUT tensor #43: “sequential_1_1/sequential_1/batch_normalization_1_2/batchnorm/mul_1” shape=(1, 200, 200, 64), type=int8, scale=0.48836034536361694, zp=-125\n",
      "\n",
      "Step 39: DELEGATE\n",
      "    IN  tensor #30: “tfl.pseudo_qconst28” shape=(64,), type=int8, scale=0.0061034648679196835, zp=127\n",
      "    IN  tensor #43: “sequential_1_1/sequential_1/batch_normalization_1_2/batchnorm/mul_1” shape=(1, 200, 200, 64), type=int8, scale=0.48836034536361694, zp=-125\n",
      "    OUT tensor #44: “sequential_1_1/sequential_1/batch_normalization_1_2/batchnorm/add_1” shape=(1, 200, 200, 64), type=int8, scale=0.487743616104126, zp=-125\n",
      "\n",
      "Step 40: DELEGATE\n",
      "    IN  tensor #44: “sequential_1_1/sequential_1/batch_normalization_1_2/batchnorm/add_1” shape=(1, 200, 200, 64), type=int8, scale=0.487743616104126, zp=-125\n",
      "    OUT tensor #45: “sequential_1_1/sequential_1/max_pooling2d_1/MaxPool2d” shape=(1, 100, 100, 64), type=int8, scale=0.487743616104126, zp=-125\n",
      "\n",
      "Step 41: DELEGATE\n",
      "    IN  tensor #28: “tfl.pseudo_qconst26” shape=(128,), type=int32, scale=0.0, zp=0\n",
      "    IN  tensor #29: “tfl.pseudo_qconst27” shape=(128, 3, 3, 64), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #45: “sequential_1_1/sequential_1/max_pooling2d_1/MaxPool2d” shape=(1, 100, 100, 64), type=int8, scale=0.487743616104126, zp=-125\n",
      "    OUT tensor #46: “sequential_1_1/sequential_1/conv2d_2_1/Relu;sequential_1_1/sequential_1/conv2d_2_1/BiasAdd;sequential_1_1/sequential_1/conv2d_2_1/convolution;” shape=(1, 100, 100, 128), type=int8, scale=0.5958064198493958, zp=-128\n",
      "\n",
      "Step 42: DELEGATE\n",
      "    IN  tensor #27: “tfl.pseudo_qconst25” shape=(128,), type=int8, scale=0.001440622960217297, zp=-128\n",
      "    IN  tensor #46: “sequential_1_1/sequential_1/conv2d_2_1/Relu;sequential_1_1/sequential_1/conv2d_2_1/BiasAdd;sequential_1_1/sequential_1/conv2d_2_1/convolution;” shape=(1, 100, 100, 128), type=int8, scale=0.5958064198493958, zp=-128\n",
      "    OUT tensor #47: “sequential_1_1/sequential_1/batch_normalization_2_1/batchnorm/mul_1” shape=(1, 100, 100, 128), type=int8, scale=0.1463908702135086, zp=-121\n",
      "\n",
      "Step 43: DELEGATE\n",
      "    IN  tensor #26: “tfl.pseudo_qconst24” shape=(128,), type=int8, scale=0.004118925426155329, zp=109\n",
      "    IN  tensor #47: “sequential_1_1/sequential_1/batch_normalization_2_1/batchnorm/mul_1” shape=(1, 100, 100, 128), type=int8, scale=0.1463908702135086, zp=-121\n",
      "    OUT tensor #48: “sequential_1_1/sequential_1/batch_normalization_2_1/batchnorm/add_1” shape=(1, 100, 100, 128), type=int8, scale=0.14274635910987854, zp=-121\n",
      "\n",
      "Step 44: DELEGATE\n",
      "    IN  tensor #24: “tfl.pseudo_qconst22” shape=(128,), type=int32, scale=0.0, zp=0\n",
      "    IN  tensor #25: “tfl.pseudo_qconst23” shape=(128, 3, 3, 128), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #48: “sequential_1_1/sequential_1/batch_normalization_2_1/batchnorm/add_1” shape=(1, 100, 100, 128), type=int8, scale=0.14274635910987854, zp=-121\n",
      "    OUT tensor #49: “sequential_1_1/sequential_1/conv2d_3_1/Relu;sequential_1_1/sequential_1/conv2d_3_1/BiasAdd;sequential_1_1/sequential_1/conv2d_3_1/convolution;” shape=(1, 100, 100, 128), type=int8, scale=0.9574838280677795, zp=-128\n",
      "\n",
      "Step 45: DELEGATE\n",
      "    IN  tensor #23: “tfl.pseudo_qconst21” shape=(128,), type=int8, scale=0.001115808729082346, zp=-128\n",
      "    IN  tensor #49: “sequential_1_1/sequential_1/conv2d_3_1/Relu;sequential_1_1/sequential_1/conv2d_3_1/BiasAdd;sequential_1_1/sequential_1/conv2d_3_1/convolution;” shape=(1, 100, 100, 128), type=int8, scale=0.9574838280677795, zp=-128\n",
      "    OUT tensor #50: “sequential_1_1/sequential_1/batch_normalization_3_1/batchnorm/mul_1” shape=(1, 100, 100, 128), type=int8, scale=0.09289126843214035, zp=-117\n",
      "\n",
      "Step 46: DELEGATE\n",
      "    IN  tensor #22: “tfl.pseudo_qconst20” shape=(128,), type=int8, scale=0.004105701111257076, zp=127\n",
      "    IN  tensor #50: “sequential_1_1/sequential_1/batch_normalization_3_1/batchnorm/mul_1” shape=(1, 100, 100, 128), type=int8, scale=0.09289126843214035, zp=-117\n",
      "    OUT tensor #51: “sequential_1_1/sequential_1/batch_normalization_3_1/batchnorm/add_1” shape=(1, 100, 100, 128), type=int8, scale=0.09052053838968277, zp=-116\n",
      "\n",
      "Step 47: DELEGATE\n",
      "    IN  tensor #51: “sequential_1_1/sequential_1/batch_normalization_3_1/batchnorm/add_1” shape=(1, 100, 100, 128), type=int8, scale=0.09052053838968277, zp=-116\n",
      "    OUT tensor #52: “sequential_1_1/sequential_1/max_pooling2d_1_2/MaxPool2d” shape=(1, 50, 50, 128), type=int8, scale=0.09052053838968277, zp=-116\n",
      "\n",
      "Step 48: DELEGATE\n",
      "    IN  tensor #20: “tfl.pseudo_qconst18” shape=(256,), type=int32, scale=0.0, zp=0\n",
      "    IN  tensor #21: “tfl.pseudo_qconst19” shape=(256, 3, 3, 128), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #52: “sequential_1_1/sequential_1/max_pooling2d_1_2/MaxPool2d” shape=(1, 50, 50, 128), type=int8, scale=0.09052053838968277, zp=-116\n",
      "    OUT tensor #53: “sequential_1_1/sequential_1/conv2d_4_1/Relu;sequential_1_1/sequential_1/conv2d_4_1/BiasAdd;sequential_1_1/sequential_1/conv2d_4_1/convolution;” shape=(1, 50, 50, 256), type=int8, scale=0.9490053653717041, zp=-128\n",
      "\n",
      "Step 49: DELEGATE\n",
      "    IN  tensor #19: “tfl.pseudo_qconst17” shape=(256,), type=int8, scale=0.0010407568188384175, zp=-128\n",
      "    IN  tensor #53: “sequential_1_1/sequential_1/conv2d_4_1/Relu;sequential_1_1/sequential_1/conv2d_4_1/BiasAdd;sequential_1_1/sequential_1/conv2d_4_1/convolution;” shape=(1, 50, 50, 256), type=int8, scale=0.9490053653717041, zp=-128\n",
      "    OUT tensor #54: “sequential_1_1/sequential_1/batch_normalization_4_1/batchnorm/mul_1” shape=(1, 50, 50, 256), type=int8, scale=0.11567893624305725, zp=-114\n",
      "\n",
      "Step 50: DELEGATE\n",
      "    IN  tensor #18: “tfl.pseudo_qconst16” shape=(256,), type=int8, scale=0.008151660673320293, zp=78\n",
      "    IN  tensor #54: “sequential_1_1/sequential_1/batch_normalization_4_1/batchnorm/mul_1” shape=(1, 50, 50, 256), type=int8, scale=0.11567893624305725, zp=-114\n",
      "    OUT tensor #55: “sequential_1_1/sequential_1/batch_normalization_4_1/batchnorm/add_1” shape=(1, 50, 50, 256), type=int8, scale=0.11381451785564423, zp=-113\n",
      "\n",
      "Step 51: DELEGATE\n",
      "    IN  tensor #16: “tfl.pseudo_qconst14” shape=(256,), type=int32, scale=0.0, zp=0\n",
      "    IN  tensor #17: “tfl.pseudo_qconst15” shape=(256, 3, 3, 256), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #55: “sequential_1_1/sequential_1/batch_normalization_4_1/batchnorm/add_1” shape=(1, 50, 50, 256), type=int8, scale=0.11381451785564423, zp=-113\n",
      "    OUT tensor #56: “sequential_1_1/sequential_1/conv2d_5_1/Relu;sequential_1_1/sequential_1/conv2d_5_1/BiasAdd;sequential_1_1/sequential_1/conv2d_5_1/convolution;” shape=(1, 50, 50, 256), type=int8, scale=1.2002378702163696, zp=-128\n",
      "\n",
      "Step 52: DELEGATE\n",
      "    IN  tensor #15: “tfl.pseudo_qconst13” shape=(256,), type=int8, scale=0.09097450226545334, zp=-128\n",
      "    IN  tensor #56: “sequential_1_1/sequential_1/conv2d_5_1/Relu;sequential_1_1/sequential_1/conv2d_5_1/BiasAdd;sequential_1_1/sequential_1/conv2d_5_1/convolution;” shape=(1, 50, 50, 256), type=int8, scale=1.2002378702163696, zp=-128\n",
      "    OUT tensor #57: “sequential_1_1/sequential_1/batch_normalization_5_1/batchnorm/mul_1” shape=(1, 50, 50, 256), type=int8, scale=0.3943060338497162, zp=-126\n",
      "\n",
      "Step 53: DELEGATE\n",
      "    IN  tensor #14: “tfl.pseudo_qconst12” shape=(256,), type=int8, scale=0.003829863155260682, zp=87\n",
      "    IN  tensor #57: “sequential_1_1/sequential_1/batch_normalization_5_1/batchnorm/mul_1” shape=(1, 50, 50, 256), type=int8, scale=0.3943060338497162, zp=-126\n",
      "    OUT tensor #58: “sequential_1_1/sequential_1/batch_normalization_5_1/batchnorm/add_1” shape=(1, 50, 50, 256), type=int8, scale=0.39389708638191223, zp=-126\n",
      "\n",
      "Step 54: DELEGATE\n",
      "    IN  tensor #58: “sequential_1_1/sequential_1/batch_normalization_5_1/batchnorm/add_1” shape=(1, 50, 50, 256), type=int8, scale=0.39389708638191223, zp=-126\n",
      "    OUT tensor #59: “sequential_1_1/sequential_1/max_pooling2d_2_1/MaxPool2d” shape=(1, 25, 25, 256), type=int8, scale=0.39389708638191223, zp=-126\n",
      "\n",
      "Step 55: DELEGATE\n",
      "    IN  tensor #12: “tfl.pseudo_qconst10” shape=(512,), type=int32, scale=0.0, zp=0\n",
      "    IN  tensor #13: “tfl.pseudo_qconst11” shape=(512, 3, 3, 256), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #59: “sequential_1_1/sequential_1/max_pooling2d_2_1/MaxPool2d” shape=(1, 25, 25, 256), type=int8, scale=0.39389708638191223, zp=-126\n",
      "    OUT tensor #60: “sequential_1_1/sequential_1/conv2d_6_1/Relu;sequential_1_1/sequential_1/conv2d_6_1/BiasAdd;sequential_1_1/sequential_1/conv2d_6_1/convolution;” shape=(1, 25, 25, 512), type=int8, scale=1.5455434322357178, zp=-128\n",
      "\n",
      "Step 56: DELEGATE\n",
      "    IN  tensor #11: “tfl.pseudo_qconst9” shape=(512,), type=int8, scale=0.0017053347546607256, zp=-128\n",
      "    IN  tensor #60: “sequential_1_1/sequential_1/conv2d_6_1/Relu;sequential_1_1/sequential_1/conv2d_6_1/BiasAdd;sequential_1_1/sequential_1/conv2d_6_1/convolution;” shape=(1, 25, 25, 512), type=int8, scale=1.5455434322357178, zp=-128\n",
      "    OUT tensor #61: “sequential_1_1/sequential_1/batch_normalization_6_1/batchnorm/mul_1” shape=(1, 25, 25, 512), type=int8, scale=0.15267038345336914, zp=-124\n",
      "\n",
      "Step 57: DELEGATE\n",
      "    IN  tensor #10: “tfl.pseudo_qconst8” shape=(512,), type=int8, scale=0.003270635614171624, zp=70\n",
      "    IN  tensor #61: “sequential_1_1/sequential_1/batch_normalization_6_1/batchnorm/mul_1” shape=(1, 25, 25, 512), type=int8, scale=0.15267038345336914, zp=-124\n",
      "    OUT tensor #62: “sequential_1_1/sequential_1/batch_normalization_6_1/batchnorm/add_1” shape=(1, 25, 25, 512), type=int8, scale=0.15163584053516388, zp=-124\n",
      "\n",
      "Step 58: DELEGATE\n",
      "    IN  tensor #8: “tfl.pseudo_qconst6” shape=(512,), type=int32, scale=0.0, zp=0\n",
      "    IN  tensor #9: “tfl.pseudo_qconst7” shape=(512, 3, 3, 512), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #62: “sequential_1_1/sequential_1/batch_normalization_6_1/batchnorm/add_1” shape=(1, 25, 25, 512), type=int8, scale=0.15163584053516388, zp=-124\n",
      "    OUT tensor #63: “sequential_1_1/sequential_1/conv2d_7_1/Relu;sequential_1_1/sequential_1/conv2d_7_1/BiasAdd;sequential_1_1/sequential_1/conv2d_7_1/convolution;” shape=(1, 25, 25, 512), type=int8, scale=1.0951658487319946, zp=-128\n",
      "\n",
      "Step 59: DELEGATE\n",
      "    IN  tensor #7: “tfl.pseudo_qconst5” shape=(512,), type=int8, scale=0.1490822583436966, zp=-128\n",
      "    IN  tensor #63: “sequential_1_1/sequential_1/conv2d_7_1/Relu;sequential_1_1/sequential_1/conv2d_7_1/BiasAdd;sequential_1_1/sequential_1/conv2d_7_1/convolution;” shape=(1, 25, 25, 512), type=int8, scale=1.0951658487319946, zp=-128\n",
      "    OUT tensor #64: “sequential_1_1/sequential_1/batch_normalization_7_1/batchnorm/mul_1” shape=(1, 25, 25, 512), type=int8, scale=0.6881277561187744, zp=-126\n",
      "\n",
      "Step 60: DELEGATE\n",
      "    IN  tensor #6: “tfl.pseudo_qconst4” shape=(512,), type=int8, scale=0.005283291917294264, zp=113\n",
      "    IN  tensor #64: “sequential_1_1/sequential_1/batch_normalization_7_1/batchnorm/mul_1” shape=(1, 25, 25, 512), type=int8, scale=0.6881277561187744, zp=-126\n",
      "    OUT tensor #65: “sequential_1_1/sequential_1/batch_normalization_7_1/batchnorm/add_1” shape=(1, 25, 25, 512), type=int8, scale=0.6877288222312927, zp=-126\n",
      "\n",
      "Step 61: DELEGATE\n",
      "    IN  tensor #65: “sequential_1_1/sequential_1/batch_normalization_7_1/batchnorm/add_1” shape=(1, 25, 25, 512), type=int8, scale=0.6877288222312927, zp=-126\n",
      "    OUT tensor #66: “sequential_1_1/sequential_1/max_pooling2d_3_1/MaxPool2d” shape=(1, 12, 12, 512), type=int8, scale=0.6877288222312927, zp=-126\n",
      "\n",
      "Step 62: DELEGATE\n",
      "    IN  tensor #1: “arith.constant” shape=(2,), type=int32, scale=0.0, zp=0\n",
      "    IN  tensor #66: “sequential_1_1/sequential_1/max_pooling2d_3_1/MaxPool2d” shape=(1, 12, 12, 512), type=int8, scale=0.6877288222312927, zp=-126\n",
      "    OUT tensor #67: “sequential_1_1/sequential_1/flatten_1/Reshape” shape=(1, 73728), type=int8, scale=0.6877288222312927, zp=-126\n",
      "\n",
      "Step 63: DELEGATE\n",
      "    IN  tensor #4: “tfl.pseudo_qconst2” shape=(512,), type=int32, scale=0.0, zp=0\n",
      "    IN  tensor #5: “tfl.pseudo_qconst3” shape=(512, 73728), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #67: “sequential_1_1/sequential_1/flatten_1/Reshape” shape=(1, 73728), type=int8, scale=0.6877288222312927, zp=-126\n",
      "    OUT tensor #68: “sequential_1_1/sequential_1/dense_1/MatMul;sequential_1_1/sequential_1/dense_1/Relu;sequential_1_1/sequential_1/dense_1/BiasAdd” shape=(1, 512), type=int8, scale=4.46315336227417, zp=-128\n",
      "\n",
      "Step 64: DELEGATE\n",
      "    IN  tensor #2: “tfl.pseudo_qconst” shape=(29,), type=int32, scale=0.0, zp=0\n",
      "    IN  tensor #3: “tfl.pseudo_qconst1” shape=(29, 512), type=int8, scale=0.0, zp=0\n",
      "    IN  tensor #68: “sequential_1_1/sequential_1/dense_1/MatMul;sequential_1_1/sequential_1/dense_1/Relu;sequential_1_1/sequential_1/dense_1/BiasAdd” shape=(1, 512), type=int8, scale=4.46315336227417, zp=-128\n",
      "    OUT tensor #69: “sequential_1_1/sequential_1/batch_normalization_8_1/batchnorm/mul_1;sequential_1_1/sequential_1/batch_normalization_8_1/batchnorm/add_1;sequential_1_1/sequential_1/logits_1/MatMul;sequential_1_1/sequential_1/logits_1/BiasAdd” shape=(1, 29), type=int8, scale=0.3223066031932831, zp=-18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tflite_runtime.interpreter import Interpreter   # <— note the change!\n",
    "\n",
    "# 1) Load the interpreter WITHOUT any delegates and preserve intermediates\n",
    "interp = Interpreter(\n",
    "    model_path=\"asl_model_quantized_int8.tflite\",\n",
    "    experimental_preserve_all_tensors=True\n",
    "    # no 'experimental_delegates' here – tflite-runtime has none by default\n",
    ")\n",
    "interp.allocate_tensors()\n",
    "\n",
    "# 2) Build a dict of tensor details\n",
    "tensor_details = { d[\"index\"]: d for d in interp.get_tensor_details() }\n",
    "\n",
    "# 3) Fetch the raw operator list\n",
    "ops = interp._get_ops_details()\n",
    "\n",
    "print(\"\\nExecution steps (pure‑TFLite CPU ops):\\n\")\n",
    "for step, op in enumerate(ops):\n",
    "    print(f\"Step {step:2d}: {op['op_name']}\")\n",
    "    def desc(tidx, role):\n",
    "        d = tensor_details[tidx]\n",
    "        shape = tuple(d[\"shape\"])\n",
    "        dtype = np.dtype(d[\"dtype\"]).name\n",
    "        q = d.get(\"quantization\")\n",
    "        if q:\n",
    "            scale, zp = q\n",
    "        else:\n",
    "            scales = d[\"quantization_parameters\"][\"scales\"]\n",
    "            zps    = d[\"quantization_parameters\"][\"zero_points\"]\n",
    "            scale = float(scales[0]) if len(scales)>0 else None\n",
    "            zp    = int(zps[0])    if len(zps)>0    else None\n",
    "        return f\"    {role} tensor #{tidx}: “{d['name']}” shape={shape}, type={dtype}, scale={scale}, zp={zp}\"\n",
    "    for tid in op[\"inputs\"]:\n",
    "        print(desc(tid, \"IN \"))\n",
    "    for tid in op[\"outputs\"]:\n",
    "        print(desc(tid, \"OUT\"))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49620e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 71 tensors to data/model_dump.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "def sanitize(name):\n",
    "    return re.sub(r'[^0-9a-zA-Z_]', '_', name)\n",
    "\n",
    "# 1) Load & prepare the interpreter\n",
    "interp = Interpreter(\n",
    "    model_path=\"asl_model_quantized_int8.tflite\",\n",
    "    experimental_preserve_all_tensors=True\n",
    ")\n",
    "interp.allocate_tensors()\n",
    "\n",
    "# 2) Load your JPEG, resize to 200×200, and set it as the input\n",
    "img = Image.open(\"asl_alphabet_test/C_test.jpg\").convert(\"RGB\").resize((200,200))\n",
    "input_arr = np.array(img, dtype=np.uint8)[None, ...]   # shape (1,200,200,3)\n",
    "\n",
    "in_idx = interp.get_input_details()[0][\"index\"]\n",
    "interp.set_tensor(in_idx, input_arr)\n",
    "\n",
    "# 3) Run the graph so QUANTIZE and all other ops execute\n",
    "interp.invoke()\n",
    "\n",
    "# 4) Now dump to JSON\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "model_json = {}\n",
    "\n",
    "for detail in interp.get_tensor_details():\n",
    "    name   = detail[\"name\"]\n",
    "    scales = detail[\"quantization_parameters\"][\"scales\"]\n",
    "    zps    = detail[\"quantization_parameters\"][\"zero_points\"]\n",
    "\n",
    "    # only keep truly‑quantized tensors\n",
    "    if scales.size == 0 or scales[0] == 0.0:\n",
    "        continue\n",
    "\n",
    "    idx = detail[\"index\"]\n",
    "    try:\n",
    "        arr = interp.get_tensor(idx)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    entry = {\n",
    "      \"shape\":      [int(d) for d in detail[\"shape\"]],\n",
    "      \"dtype\":      np.dtype(detail[\"dtype\"]).name,\n",
    "      \"scale\":      float(scales[0]),\n",
    "      \"zero_point\": int(zps[0]),\n",
    "      \"data\":       arr.flatten().tolist()\n",
    "    }\n",
    "    model_json[name] = entry\n",
    "\n",
    "with open(\"data/model_dump.json\", \"w\") as f:\n",
    "    json.dump(model_json, f, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(model_json)} tensors to data/model_dump.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "725e57eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 values:\n",
      "  Python: 54, 38, 12\n",
      "   C++  : -74, -90, -116\n",
      "Last  3 values:\n",
      "  Python: -60, -69, -79\n",
      "   C++  : 68, 59, 49\n",
      "tfl.quantize\n",
      "Total elements : 120000\n",
      "Mismatches     : 120000\n",
      "Maximum error  : 128\n",
      "\n",
      "First mismatch at index 0:\n",
      "  Python: 54 (0x36)\n",
      "  C++   : -74 (0xB6)\n"
     ]
    }
   ],
   "source": [
    "py_out = None\n",
    "# 1) Load the C++ output\n",
    "import numpy as np\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "# 1) Load the C++ output\n",
    "cpp_path = \"../CPP_Model/cpp_out/tfl_quantize_output.bin\"\n",
    "cpp_out  = np.fromfile(cpp_path, dtype=np.int8)\n",
    "\n",
    "# 2) Run the Python TFLite quantize op\n",
    "interp = Interpreter(\n",
    "    model_path=\"asl_model_quantized_int8.tflite\",\n",
    "    experimental_preserve_all_tensors=True\n",
    ")\n",
    "interp.allocate_tensors()\n",
    "\n",
    "# (Re‑set the same input as you used in C++)\n",
    "from PIL import Image\n",
    "img = Image.open(\"asl_alphabet_test/C_test.jpg\").convert(\"RGB\").resize((200,200))\n",
    "inp = np.array(img, np.uint8)[None,...]\n",
    "interp.set_tensor(interp.get_input_details()[0][\"index\"], inp)\n",
    "\n",
    "interp.invoke()\n",
    "\n",
    "# find the tfl.quantize tensor\n",
    "py_out = None\n",
    "for d in interp.get_tensor_details():\n",
    "    if d[\"name\"] == \"tfl.quantize\":\n",
    "        py_out = interp.get_tensor(d[\"index\"]).astype(np.int8).flatten()\n",
    "        break\n",
    "assert py_out is not None, \"tfl.quantize tensor not found\"\n",
    "\n",
    "# 3) Print first and last 3 values\n",
    "def fmt(arr):\n",
    "    return \", \".join(f\"{int(x)}\" for x in arr)\n",
    "\n",
    "print(\"First 3 values:\")\n",
    "print(f\"  Python: {fmt(py_out[:3])}\")\n",
    "print(f\"   C++  : {fmt(cpp_out[:3])}\")\n",
    "\n",
    "print(\"Last  3 values:\")\n",
    "print(f\"  Python: {fmt(py_out[-3:])}\")\n",
    "print(f\"   C++  : {fmt(cpp_out[-3:])}\")\n",
    "\n",
    "cpp_out  = np.fromfile(cpp_path, dtype=np.int8)\n",
    "\n",
    "for d in interp.get_tensor_details():\n",
    "    if d[\"name\"] == \"tfl.quantize\":\n",
    "        print(d[\"name\"])\n",
    "        py_out = interp.get_tensor(d[\"index\"]).astype(np.int8).flatten()\n",
    "        break\n",
    "assert py_out is not None, \"tfl.quantize tensor not found\"\n",
    "\n",
    "\n",
    "# 3) Check shapes match\n",
    "if py_out.shape != cpp_out.shape:\n",
    "    raise ValueError(f\"Shape mismatch: python {py_out.shape}, cpp {cpp_out.shape}\")\n",
    "\n",
    "# 4) Compare\n",
    "diff = py_out.astype(int) - cpp_out.astype(int)\n",
    "n_mismatch = np.count_nonzero(diff)\n",
    "max_err    = np.max(np.abs(diff)) if n_mismatch else 0\n",
    "\n",
    "print(f\"Total elements : {py_out.size}\")\n",
    "print(f\"Mismatches     : {n_mismatch}\")\n",
    "print(f\"Maximum error  : {max_err}\")\n",
    "\n",
    "if n_mismatch:\n",
    "    idx0 = np.nonzero(diff)[0][0]\n",
    "    print(f\"\\nFirst mismatch at index {idx0}:\")\n",
    "    print(f\"  Python: {py_out[idx0]} (0x{py_out[idx0]&0xFF:02X})\")\n",
    "    print(f\"  C++   : {cpp_out[idx0]} (0x{cpp_out[idx0]&0xFF:02X})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72ffdcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First  3 ref : [182, 166, 140]   cpp : [182, 166, 140]\n",
      "Last   3 ref : [68, 59, 49]   cpp : [68, 59, 49]\n",
      "Total elements : 120000\n",
      "Matches        : 120000\n",
      "Mismatches     : 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# 1) Load the JSON dump\n",
    "with open(\"data/model_dump.json\") as f:\n",
    "    model = json.load(f)\n",
    "\n",
    "# 2) Extract the quantized input array\n",
    "inp_meta = model[\"serving_default_keras_tensor_30:0\"]\n",
    "ref = np.array(inp_meta[\"data\"], dtype=np.uint8)\n",
    "\n",
    "\n",
    "# 3) Load your C++ output\n",
    "cpp = np.fromfile(\"../CPP_Model/cpp_out/tfl_quantize_output.bin\", dtype=np.uint8)\n",
    "\n",
    "print(\"First  3 ref :\", ref[:3].tolist(), \"  cpp :\", cpp[:3].tolist())\n",
    "print(\"Last   3 ref :\", ref[-3:].tolist(), \"  cpp :\", cpp[-3:].tolist())\n",
    "\n",
    "matches    = ref == cpp\n",
    "n_total    = ref.size\n",
    "n_matches  = np.count_nonzero(matches)\n",
    "n_mismatch = n_total - n_matches\n",
    "\n",
    "print(f\"Total elements : {n_total}\")\n",
    "print(f\"Matches        : {n_matches}\")\n",
    "print(f\"Mismatches     : {n_mismatch}\")\n",
    "\n",
    "\n",
    "# 4) Compare\n",
    "# mismatches = np.nonzero(raw != q)[0]\n",
    "# print(f\"\\nMismatches count: {len(mismatches)}\")\n",
    "# if len(mismatches):\n",
    "#     i = mismatches[0]\n",
    "#     print(f\" First mismatch at idx {i}: json={raw[i]}, calc={q[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
